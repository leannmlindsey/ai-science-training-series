## Homework 7 outputs

### Cerebras outputs
https://github.com/leannmlindsey/ai-science-training-series/blob/main/07_largeScaleTraining/mytest_1024.log

https://github.com/leannmlindsey/ai-science-training-series/blob/main/07_largeScaleTraining/mytest_512.log

https://github.com/leannmlindsey/ai-science-training-series/blob/main/07_largeScaleTraining/mytest_2048.log

Observations: The batch size of 512 finished faster than the batch size of 1024, while the batch size of 2048 had an error and was unable to finish.

### Graphcore outputs
https://github.com/leannmlindsey/ai-science-training-series/blob/main/07_largeScaleTraining/homework7.log

Best accuracy was with the following parameters:
Accuracy on test set: 98.56%

Hyper-parameter changes were
learning_rate = 0.01
epochs = 25
batch_size = 32
test_batch_size = 80 

### Sambanova outputs
https://github.com/leannmlindsey/ai-science-training-series/blob/main/07_largeScaleTraining/slurm-final-30959.out

https://github.com/leannmlindsey/ai-science-training-series/blob/main/07_largeScaleTraining/slurm-final-4-30961.out

### Groq outputs
https://github.com/leannmlindsey/ai-science-training-series/blob/main/07_largeScaleTraining/groq_output.txt
